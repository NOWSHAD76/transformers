{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gemma3_with_talker.configuration_gemma3_with_talker import (\n",
    "    Gemma3WithTalkerConfig,\n",
    ")\n",
    "from transformers.models.gemma3_with_talker.modeling_gemma3_with_talker import (\n",
    "    Gemma3WithTalkerForConditionalGeneration,\n",
    "    Gemma3WithTalkerThinkerForCausalLM,\n",
    ")\n",
    "from transformers.models.gemma3_with_talker.configuration_gemma3_with_talker import *\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "from loguru import logger\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# from loguru import logger\n",
    "# from transformers import AutoConfig\n",
    "\n",
    "from transformers.models.gemma3_with_talker.configuration_gemma3_with_talker import (\n",
    "    Gemma3WithTalkerThinkerTextConfig,\n",
    ")\n",
    "from transformers.models.gemma3_with_talker.modeling_gemma3_with_talker import (\n",
    "    Gemma3WithTalkerThinkerTextModel,\n",
    ")\n",
    "\n",
    "configs = {\n",
    "    # \"gemma3_thinker\": Gemma3ThinkerConfig,\n",
    "    # \"gemma3_talker\": Gemma3TalkerConfig,\n",
    "    # \"gemma3_token2wav\": Gemma3Token2WavConfig,\n",
    "    # \"gemma3_with_talker\": Gemma3WithTalkerConfig,\n",
    "    \"gemma3_with_talker_thinker_text\": Gemma3WithTalkerThinkerTextConfig,\n",
    "    # \"gemma3_bigvgan\": Gemma3BigVGANConfig,\n",
    "    # \"gemma3_dit\": Gemma3DiTConfig,\n",
    "}\n",
    "for name, conf in configs.items():\n",
    "    AutoConfig.register(name, conf)\n",
    "\n",
    "# AutoConfig.register(\"gemma3_with_talker\", Gemma3WithTalkerConfig)\n",
    "AutoModel.register(Gemma3WithTalkerThinkerTextConfig, Gemma3WithTalkerThinkerTextModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LIBRARY_PATH\"] = \"/usr/local/cuda/lib64/stubs:$LIBRARY_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-02 11:39:02.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtransformers.models.gemma3_with_talker.modeling_gemma3_with_talker\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m4827\u001b[0m - \u001b[1mInside from pretrained\u001b[0m\n",
      "\u001b[32m2025-06-02 11:39:02.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtransformers.models.gemma3_with_talker.modeling_gemma3_with_talker\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m4828\u001b[0m - \u001b[34m\u001b[1mConfig None\u001b[0m\n",
      "\u001b[32m2025-06-02 11:39:02.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtransformers.models.gemma3_with_talker.modeling_gemma3_with_talker\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m4829\u001b[0m - \u001b[34m\u001b[1mModel args\u001b[0m\n",
      "\u001b[32m2025-06-02 11:39:02.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtransformers.models.gemma3_with_talker.modeling_gemma3_with_talker\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m4831\u001b[0m - \u001b[34m\u001b[1mkwargs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside tie weights\n",
      "Inside tie weights getattr tie_word_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e095a97cc8a547789ef1accc65fa09a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside tie weights\n",
      "Inside tie weights getattr tie_word_embeddings\n"
     ]
    }
   ],
   "source": [
    "model = Gemma3WithTalkerForConditionalGeneration.from_pretrained(\n",
    "    # \"/data1/nowshad/models/gemma3_with_talker_base\",\n",
    "    \"/data1/nowshad/models/gemma3_with_talker_merge\",\n",
    "    device_map=\"cuda:6\",\n",
    "    # config=config,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    # ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([262145, 3840])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"thinker.model.lm_head.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3840, out_features=262145, bias=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "lm_head = nn.Linear(3840, 262145, bias=False)\n",
    "lm_head.weight = nn.Parameter(tensors[\"thinker.model.lm_head.weight\"])\n",
    "lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Parameter.values>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(tensors[\"thinker.model.lm_head.weight\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0964e-03, -3.8910e-04, -3.7994e-03,  ...,  1.8234e-03,\n",
       "          1.5259e-03, -5.0659e-03],\n",
       "        [ 4.4556e-03,  1.4282e-02,  5.6458e-04,  ..., -1.3428e-02,\n",
       "          8.2016e-04,  9.6436e-03],\n",
       "        [ 2.7222e-02, -1.1902e-03, -2.0020e-02,  ...,  8.5449e-03,\n",
       "          1.6113e-02,  7.3242e-04],\n",
       "        ...,\n",
       "        [-3.7384e-03, -2.0447e-03, -3.6316e-03,  ...,  1.9531e-03,\n",
       "          8.9645e-04, -4.6692e-03],\n",
       "        [-2.3041e-03, -1.6174e-03, -6.2866e-03,  ...,  2.7008e-03,\n",
       "          2.9755e-03, -5.1270e-03],\n",
       "        [-7.9155e-05, -8.5831e-05, -6.1512e-05,  ..., -7.3910e-05,\n",
       "          1.2040e-05,  2.9802e-06]], device='cuda:6', dtype=torch.bfloat16,\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thinker_model.lm_head.weight.to(\"cuda:6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0964e-03, -3.8910e-04, -3.7994e-03,  ...,  1.8234e-03,\n",
       "          1.5259e-03, -5.0659e-03],\n",
       "        [ 4.4556e-03,  1.4282e-02,  5.6458e-04,  ..., -1.3428e-02,\n",
       "          8.2016e-04,  9.6436e-03],\n",
       "        [ 2.7222e-02, -1.1902e-03, -2.0020e-02,  ...,  8.5449e-03,\n",
       "          1.6113e-02,  7.3242e-04],\n",
       "        ...,\n",
       "        [-3.7384e-03, -2.0447e-03, -3.6316e-03,  ...,  1.9531e-03,\n",
       "          8.9645e-04, -4.6692e-03],\n",
       "        [-2.3041e-03, -1.6174e-03, -6.2866e-03,  ...,  2.7008e-03,\n",
       "          2.9755e-03, -5.1270e-03],\n",
       "        [-7.9155e-05, -8.5831e-05, -6.1512e-05,  ..., -7.3910e-05,\n",
       "          1.2040e-05,  2.9802e-06]], device='cuda:6', dtype=torch.bfloat16,\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = thinker_model.lm_head.weight.to(\"cuda:6\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.thinker.lm_head.weight = thinker_model.lm_head.weight\n",
    "# model.thinker.lm_head.weight = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thinker_model = Gemma3WithTalkerThinkerForCausalLM.from_pretrained(\n",
    "#     \"/data1/nowshad/models/gemma3_with_talker_base\",\n",
    "#     # \"/data1/nowshad/models/gemma3_with_talker_merge\",\n",
    "#     device_map=\"cuda:6\",\n",
    "#     # config=config,\n",
    "#     torch_dtype=\"bfloat16\",\n",
    "#     # ignore_mismatched_sizes=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside tie weights\n",
      "Inside tie weights getattr tie_word_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02e1b2f48b44607aa86f5369f3a77b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside tie weights\n",
      "Inside tie weights getattr tie_word_embeddings\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForImageTextToText,\n",
    ")\n",
    "\n",
    "thinker_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"/data1/nowshad/models/gemma3_12b_ft_20250519\",\n",
    "    # \"/data1/nowshad/models/Qwen2-7B\",\n",
    "    # \"/data1/nowshad/models/Qwen2.5-VL-7B-Instruct_processor\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    device_map=\"cuda:6\",\n",
    "    # trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3840, out_features=262145, bias=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thinker_model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.thinker = thinker_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "262208 - 262145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "thinker_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data1/nowshad/models/gemma3_12b_ft_20250519\"\n",
    ")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is your name?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "text = thinker_tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# text = \"What is your name?\"\n",
    "inputs = thinker_tokenizer([text], return_tensors=\"pt\").to(\"cuda:6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:8292 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "thinker_result = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_forward_unimplemented() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m thinker_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/nowshad/rpTalker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/nowshad/rpTalker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: _forward_unimplemented() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "# thinker_result = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ids, audio = thinker_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user\\nYou are a helpful assistant.\\n\\nWhat is your name?\\nmodel\\nMy name is Gemma! I'm a large language model created by the Gemma team at Google DeepMind. It's nice to meet you. ðŸ˜Š\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thinker_tokenizer.batch_decode(text_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "sf.write(\n",
    "    \"output.wav\",\n",
    "    audio.reshape(-1).detach().cpu().numpy(),\n",
    "    samplerate=24000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/data1/nowshad/models/gemma3_with_talker_merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "\n",
    "def convert_tensor(file_path):\n",
    "    tensors = {}\n",
    "    with safe_open(file_path, framework=\"pt\", device=6) as f:\n",
    "        for k in f.keys():\n",
    "            if \"language_model.model\" in k:\n",
    "                new_k = k.replace(\n",
    "                    \"language_model.model\", \"thinker.model.language_model\"\n",
    "                )\n",
    "            elif \"multi_modal_projector\" in k:\n",
    "                new_k = \"thinker.model.\" + k\n",
    "            elif \"vision_tower\" in k:\n",
    "                new_k = \"thinker.model.\" + k\n",
    "            else:\n",
    "                new_k = k\n",
    "            tensors[new_k] = f.get_tensor(k)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on model-00001-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 1/12 [00:08<01:30,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on model-00002-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:29,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on modelo-00004-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:32,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on model-00004-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:30<00:32,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on modelo-00005-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:33<00:23,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on model-00005-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:39<00:06,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on model-00003-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:46<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "read_path = \"/data1/nowshad/models/gemma3_with_talker_base\"\n",
    "save_path = \"/data1/nowshad/models/gemma3_with_talker_base/mdf\"\n",
    "files = os.listdir(read_path)\n",
    "for file in tqdm(files):\n",
    "    if file.endswith(\"safetensors\"):\n",
    "        print(f\"Working on {file}\")\n",
    "        new_tensor = convert_tensor(os.path.join(read_path, file))\n",
    "        save_file(new_tensor, os.path.join(save_path, file))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"something\"\n",
    "\"some\" in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed token weights shape model.embed_tokens.weight is torch.Size([151936, 4096])\n",
      "LM head weights shape lm_head.weight is torch.Size([151936, 4096])\n"
     ]
    }
   ],
   "source": [
    "lm_head_file_path = (\n",
    "    \"/data1/nowshad/models/DeepSeek-R1-0528-Qwen3-8B/model-00002-of-000002.safetensors\"\n",
    ")\n",
    "embed_file_path = (\n",
    "    \"/data1/nowshad/models/DeepSeek-R1-0528-Qwen3-8B/model-00001-of-000002.safetensors\"\n",
    ")\n",
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open(embed_file_path, framework=\"pt\", device=6) as f:\n",
    "    for k in f.keys():\n",
    "        if \"embed_tokens\" in k:\n",
    "            print(f\"Embed token weights shape {k} is {f.get_tensor(k).shape}\")\n",
    "\n",
    "with safe_open(lm_head_file_path, framework=\"pt\", device=6) as f:\n",
    "    for k in f.keys():\n",
    "        if \"lm_head\" in k:\n",
    "            print(f\"LM head weights shape {k} is {f.get_tensor(k).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'something is 25-30 and country is SG and else is Playful'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"something is {age} and country is {country} and else is {style}\"\n",
    "x.format(age=\"25-30\", country=\"SG\", style=\"Playful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"thinker.model.language_model.embed_tokens.weight\": \"model-00001-of-00005.safetensors\"\n",
    "\n",
    "\"lm_head.weight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed token weights shape thinker.model.language_model.embed_tokens.weight is torch.Size([262145, 3840])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\n",
    "    \"/data1/nowshad/models/gemma3_with_talker_base/model-00001-of-00005.safetensors\",\n",
    "    framework=\"pt\",\n",
    "    device=6,\n",
    ") as f:\n",
    "    for k in f.keys():\n",
    "        if \"embed_tokens\" in k:\n",
    "            print(f\"Embed token weights shape {k} is {f.get_tensor(k).shape}\")\n",
    "            tensors[\"thinker.model.lm_head.weight\"] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0964e-03, -3.8910e-04, -3.7994e-03,  ...,  1.8234e-03,\n",
       "          1.5259e-03, -5.0659e-03],\n",
       "        [ 4.4556e-03,  1.4282e-02,  5.6458e-04,  ..., -1.3428e-02,\n",
       "          8.2016e-04,  9.6436e-03],\n",
       "        [ 2.7222e-02, -1.1902e-03, -2.0020e-02,  ...,  8.5449e-03,\n",
       "          1.6113e-02,  7.3242e-04],\n",
       "        ...,\n",
       "        [-3.7384e-03, -2.0447e-03, -3.6316e-03,  ...,  1.9531e-03,\n",
       "          8.9645e-04, -4.6692e-03],\n",
       "        [-2.3041e-03, -1.6174e-03, -6.2866e-03,  ...,  2.7008e-03,\n",
       "          2.9755e-03, -5.1270e-03],\n",
       "        [-7.9155e-05, -8.5831e-05, -6.1512e-05,  ..., -7.3910e-05,\n",
       "          1.2040e-05,  2.9802e-06]], device='cuda:6', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"thinker.model.lm_head.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinker.model.language_model.layers.37.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.37.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.37.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.37.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.37.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.38.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.38.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.38.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.38.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.38.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.38.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.38.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.38.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.38.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.38.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.38.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.38.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.38.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.39.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.39.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.39.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.39.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.39.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.39.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.39.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.39.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.39.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.39.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.39.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.39.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.39.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.40.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.40.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.40.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.40.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.40.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.40.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.40.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.40.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.40.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.40.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.40.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.40.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.40.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.41.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.41.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.41.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.41.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.41.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.41.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.41.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.41.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.41.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.41.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.41.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.41.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.41.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.42.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.42.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.42.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.42.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.42.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.42.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.42.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.42.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.42.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.42.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.42.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.42.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.42.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.43.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.43.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.43.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.43.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.43.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.43.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.43.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.43.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.43.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.43.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.43.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.43.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.43.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.44.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.44.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.44.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.44.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.44.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.44.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.44.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.44.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.44.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.44.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.44.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.44.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.44.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.45.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.45.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.45.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.45.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.45.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.45.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.45.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.45.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.45.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.45.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.45.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.45.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.45.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.46.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.46.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.46.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.46.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.46.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.46.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.46.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.46.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.46.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.46.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.46.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.46.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.46.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.47.input_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.47.mlp.down_proj.weight shape is torch.Size([3840, 15360])\n",
      "thinker.model.language_model.layers.47.mlp.gate_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.47.mlp.up_proj.weight shape is torch.Size([15360, 3840])\n",
      "thinker.model.language_model.layers.47.post_attention_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.47.post_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.47.pre_feedforward_layernorm.weight shape is torch.Size([3840])\n",
      "thinker.model.language_model.layers.47.self_attn.k_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.47.self_attn.k_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.layers.47.self_attn.o_proj.weight shape is torch.Size([3840, 4096])\n",
      "thinker.model.language_model.layers.47.self_attn.q_norm.weight shape is torch.Size([256])\n",
      "thinker.model.language_model.layers.47.self_attn.q_proj.weight shape is torch.Size([4096, 3840])\n",
      "thinker.model.language_model.layers.47.self_attn.v_proj.weight shape is torch.Size([2048, 3840])\n",
      "thinker.model.language_model.norm.weight shape is torch.Size([3840])\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\n",
    "    \"/data1/nowshad/models/gemma3_with_talker_base/model-00005-of-00005.safetensors\",\n",
    "    framework=\"pt\",\n",
    "    device=6,\n",
    ") as f:\n",
    "    for k in f.keys():\n",
    "        # tensors[k] = f.get_tensor(k)\n",
    "        # if \"lm_head\" in k:\n",
    "        #     print(f\"Embed token weights shape {k} is {f.get_tensor(k).shape}\")\n",
    "        print(f\"{k} shape is {f.get_tensor(k).shape}\")\n",
    "        # tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensors[\"thinker.model.lm_head.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(\n",
    "    tensors,\n",
    "    \"/data1/nowshad/models/gemma3_with_talker_base/model-00005-of-00005.safetensors\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
